import redis
import json
import numpy as np
# import ollama
from sentence_transformers import SentenceTransformer
import time
import threading
import itertools
from openai import OpenAI
from dotenv import load_dotenv
import os
import tiktoken
# import sys

# åˆå§‹åŒ– Redis DBï¼ˆå–®å…ƒå‘é‡è³‡æ–™åº«ï¼‰
unit_vector_db = redis.Redis(host='localhost', port=6379, db=1)

from enum import Enum
class QuestionType(Enum):
    CHOICE = "é¸æ“‡é¡Œ"
    DEFINITION = "å®šç¾©é¡Œ"
    CALCULATION = "è¨ˆç®—é¡Œ"
    SHORT_ANSWER = "ç°¡ç­”é¡Œ"
    SITUATIONAL = "æƒ…å¢ƒé¡Œ"
    OPEN_ENDED = "é–‹æ”¾å¼å•é¡Œ"
    COMPREHENSIVE = "ç¶œåˆæ€è€ƒé¡Œ"

# è¼‰å…¥ SentenceTransformer æ¨¡å‹
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=API_KEY)
tk = tiktoken.encoding_for_model("gpt-4o-mini")


# è¼‰å…¥æ‰€æœ‰å­ä¸»é¡Œèˆ‡å‘é‡
def load_all_vectors():
    keys = unit_vector_db.keys("subtopic:*")
    subtopics = []
    vectors = []
    for key in keys:
        topic = key.decode("utf-8").replace("subtopic:", "")
        data = unit_vector_db.hget(key, "embedding")
        if data:
            embedding = np.array(json.loads(data))
            subtopics.append(topic)
            vectors.append(embedding)
    return subtopics, np.vstack(vectors)

# æ ¹æ“šè¼¸å…¥æ–‡å­—æœå°‹ç›¸ä¼¼å­ä¸»é¡Œ
def search_similar_subtopics(query, top_k=3):
    query_vec = embedding_model.encode([query], convert_to_numpy=True)
    subtopics, matrix = load_all_vectors()
    scores = np.dot(matrix, query_vec.T).reshape(-1)  # cosine ç›¸ä¼¼åº¦
    top_indices = np.argsort(scores)[-top_k:][::-1]
    return [(subtopics[i], scores[i]) for i in top_indices]

# å‹•ç•«æ——æ¨™
_loading = True

def _animate_loading(message):
    for dots in itertools.cycle(["", ".", "..", "..."]):
        if not _loading:
            break
        print(f"\r{message}{dots}   ", end="", flush=True)
        time.sleep(0.5)

def generate_random_questions_gpt4o(course_info, num_questions=3):
    global _loading
    _loading = True

    message = f"è‡ªå‹•å‡ºé¡Œä¸­"
    t = threading.Thread(target=_animate_loading, args=(message,))
    t.start()

    prompt = f"""ä½ æ˜¯ä¸€ä½è³‡æ–™çµæ§‹èª²ç¨‹çš„å‡ºé¡Œè€å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹èª²ç¨‹å…§å®¹ï¼Œ**éš¨æ©Ÿç”Ÿæˆ{num_questions} é¡Œè§€å¿µæ€§å•é¡Œ**ã€‚é€™äº›å•é¡Œéœ€ç¬¦åˆæŒ‡å®šçš„å‡ºé¡Œé¡å‹èˆ‡æ¢ä»¶ã€‚
   ---

    ## ğŸ§ ã€èª²ç¨‹å…§å®¹ã€‘ï¼š
    {course_info}

    ---

    ## ğŸ§¾ã€å‡ºé¡Œè¦ç¯„èˆ‡é™åˆ¶ã€‘ï¼š

    1. å•é¡Œå¿…é ˆèˆ‡ä¸Šè¿°èª²ç¨‹å…§å®¹æœ‰æ˜ç¢ºé—œè¯ï¼Œ**ä¸èƒ½å‡ºç¾ä»»ä½•ç¨‹å¼ç¢¼å•é¡Œ**ã€‚
    2. å•é¡Œé¡å‹é ˆå¾ä»¥ä¸‹7ç¨®é¡å‹ä¸­ç”¢ç”Ÿï¼Œä¸¦éš¨æ©Ÿé¸æ“‡ï¼š
    - 1. é¸æ“‡é¡Œï¼ˆéœ€é™„ä¸Šè§£é‡‹ç†ç”±ï¼‰
    - 2. å®šç¾©é¡Œï¼ˆè¦æ±‚å¯«å‡ºæ¦‚å¿µå®šç¾©ä¸¦èˆ‰ä¾‹ï¼‰
    - 3. è¨ˆç®—é¡Œï¼ˆè¨ˆç®—æ™‚é–“è¤‡é›œåº¦ã€æ¨¹é«˜ã€ç¢°æ’æ©Ÿç‡ç­‰ï¼‰
    - 4. ç°¡ç­”é¡Œï¼ˆå›ç­”æ¦‚å¿µä¸¦è¼”ä»¥èªªæ˜æˆ–æ¯”è¼ƒï¼‰
    - 5. æƒ…å¢ƒé¡Œï¼ˆå‡è¨­ç‰¹å®šæƒ…å¢ƒï¼Œå•å­¸ç”Ÿæœƒç™¼ç”Ÿä»€éº¼ï¼Œä¸¦èªªæ˜åŸå› ï¼‰
    - 6. é–‹æ”¾å¼å•é¡Œï¼ˆå­¸ç”Ÿå¯è‡ªç”±æå‡ºæƒ³æ³•ï¼Œä¸åªä¸€ç¨®ç­”æ¡ˆï¼‰
    - 7. ç¶œåˆæ€è€ƒé¡Œï¼ˆéœ€çµåˆå…©å€‹ä»¥ä¸Šæ¦‚å¿µå›ç­”ï¼‰

    3. é¡Œç›®éœ€ç‚º**éæ˜¯éé¡Œ**ï¼Œä¸¦ä¸”**å•é¡Œæ–‡å­—ä¸è¶…éå…©å¥è©±**ã€‚
    4. æ¯é¡Œæ‡‰å¼·èª¿**ç†è§£ã€è§£é‡‹ã€æ¯”è¼ƒã€èˆ‰ä¾‹ã€æˆ–æ¨ç†è¨ˆç®—**ã€‚
    5. é¡Œç›®ä¸­**ä¸æä¾›æç¤ºæˆ–ç¯„ä¾‹**ï¼Œä¸é™„è§£ç­”ã€‚
    6. é¡Œç›®**ä¸èƒ½åªæ˜¯çŸ¥è­˜å›æ†¶**ï¼Œæ‡‰å¼•å°å­¸ç”Ÿæ€è€ƒå…¶æ‡‰ç”¨èˆ‡ç†è§£ã€‚

    ---

    ## âœ…ã€ç¯„ä¾‹åƒè€ƒã€‘ï¼š
    - å¦‚ä½•åˆ¤å®š min-max heap ä»»ä¸€ç¯€é» [x] æ˜¯åœ¨ min æˆ– max å±¤ï¼Ÿ
    - DEAP å’Œ min-max heap çš„æƒ³æ³•æœ‰ä½•å…±é€šé»ï¼Ÿ
    - å»ºç«‹ binomial heap çš„æ™‚é–“è¤‡é›œåº¦å¦‚ä½•æ±ºå®šï¼Ÿ
    - pairing heap å’Œ Fibonacci heap æœ‰ä½•ç•°åŒï¼Ÿ
    - çµ¦å®š m ç¨®ç›¸ç•°éµå€¼ï¼Œå¦‚ä½•æ¨ç®— 2-3 æ¨¹é«˜çš„ä¸Šé™ï¼Ÿ
    - ä»¥ç·šæ€§æ¢ç´¢æœå°‹é›œæ¹Šè¡¨ä¸å­˜åœ¨å€¼çš„æ•ˆç‡å¦‚ä½•è©•ä¼°ï¼Ÿè«‹èˆ‰ä¾‹è§£èªªã€‚
    - ä»¤ m = é›œæ¹Šè¡¨å¤§å°ã€n = è³‡æ–™ç­†æ•¸ï¼Œè«‹å¾ m å’Œ n æ¨å°ç™¼ç”Ÿç¢°æ’çš„æ©Ÿç‡ã€‚
    - AVL æ¨¹åˆªé™¤è‘‰ç¯€é»å¾Œçš„æª¢æŸ¥å¯å¦æ”¹å¾æ¨¹æ ¹é–‹å§‹ç”±ä¸Šè€Œä¸‹ï¼Ÿæœ‰ä½•å„ªç¼ºé»ï¼Ÿ

    ---

    è«‹æ ¹æ“šä»¥ä¸Šè¦å‰‡ï¼Œåªç”Ÿæˆ {num_questions} é¡Œå•é¡Œã€‚æ¯é¡Œç‚ºç¹é«”ä¸­æ–‡ç¨ç«‹å•é¡Œï¼Œ*ä¸ç”¨ç·¨è™Ÿ*ç›´æ¥æ›è¡Œåˆ—å‡ºã€‚

    ---
    ## ã€è¼¸å‡ºæ ¼å¼ã€‘ï¼š
    ã€å•é¡Œé¡å‹(7ç¨®é¡å‹ä¹‹ä¸€)ã€‘æ¯é¡Œå•é¡Œç¨ç«‹ä¸€è¡Œï¼Œ**ä¸éœ€è¦ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜**ã€‚
    ã€å•é¡Œé¡å‹(7ç¨®é¡å‹ä¹‹ä¸€)ã€‘æ¯é¡Œå•é¡Œç¨ç«‹ä¸€è¡Œï¼Œ**ä¸éœ€è¦ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜**ã€‚
    ã€å•é¡Œé¡å‹(7ç¨®é¡å‹ä¹‹ä¸€)ã€‘æ¯é¡Œå•é¡Œç¨ç«‹ä¸€è¡Œï¼Œ**ä¸éœ€è¦ä»»ä½•å…¶ä»–æ–‡å­—æˆ–èªªæ˜**ã€‚

    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„èª²ç¨‹åŠ©ç†ï¼Œèƒ½æ ¹æ“šèª²ç¨‹å…§å®¹ç”Ÿæˆä¸‰å€‹ç›¸é—œå•é¡Œã€‚è«‹ç”¨ä¸­æ–‡è©¢å•ï¼Œé‡åˆ°å°ˆæœ‰åè©å¾Œé¢æ‹¬è™Ÿè‹±æ–‡åç¨±ï¼Œå¦‚:Heapï¼ˆå †ç©ï¼‰ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )

    _loading = False
    t.join()
    print("\r" + " " * (len(message) + 5), end="")  # æ¸…ç©ºå‹•ç•«è¡Œ

    return [q.strip() for q in response.choices[0].message.content.split("\n") if q.strip()]

def evaluate_answer_gpt4o(question, user_answer):
    global _loading
    _loading = True
    message = f"ğŸ“æª¢æŸ¥å›ç­”ä¸­ï¼Œè«‹è€å¿ƒç­‰å¾…"
    t = threading.Thread(target=_animate_loading, args=(message,))
    t.start()

    prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é›»è…¦ç§‘å­¸åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–è©•åˆ†ä½¿ç”¨è€…çš„å›ç­”ï¼š

### è©•åˆ†æ¨™æº–
1. 'è‹¥[ä½¿ç”¨è€…çš„å›ç­”] èˆ‡ [å•é¡Œ] ä¸ç›¸é—œï¼Œæ–‡ä¸å°é¡Œ'ç¸½åˆ†é›¶åˆ†ã€‚
2. 'è‹¥ [å•é¡Œ] è¦æ±‚èˆ‰ä¾‹ï¼Œè€Œæ²’æœ‰èˆ‰ä¾‹'ç¸½åˆ†é›¶åˆ†ã€‚
3. 'è‹¥ [å•é¡Œ] è¦æ±‚è§£é‡‹æˆ–ç†ç”±ï¼Œè€Œæœªèªªæ˜åŸå› 'ç¸½åˆ†é›¶åˆ†ã€‚
4. 'è‹¥ [å•é¡Œ] è¦æ±‚å¯«å‡ºå„ªç¼ºé»ï¼Œè€Œæœªå¯«å‡ºå„ªç¼ºé»ï¼Œæˆ–å°‘å…¶ä¸­ä¸€å€‹'ç¸½åˆ†é›¶åˆ†ã€‚
5. 'è‹¥ [ä½¿ç”¨è€…çš„å›ç­”] ç¼ºå°‘ç­”æ¡ˆé—œéµè©'æ‰£ä¸€åˆ†ã€‚
6. ' [ä½¿ç”¨è€…çš„å›ç­”] è‹¥å­—æ•¸å°‘æ–¼20å­—ï¼Œä»£è¡¨ç­”æ¡ˆéçŸ­'æ‰£ä¸‰åˆ†ï¼Œå›ç­”å¾ˆå¤šä¸æ‰£åˆ†ã€‚
7. 'è‹¥ [ä½¿ç”¨è€…çš„å›ç­”] ä¸æ­£ç¢º'æœ€é«˜åˆ†äº”åˆ†ï¼Œè‹¥è¶…éä¸€å€‹éŒ¯ï¼Œå†ä¾éŒ¯èª¤æ•¸é‡ï¼Œæ¯ç™¼ç¾ä¸€å€‹éŒ¯èª¤å°±å†æ‰£ä¸€åˆ†ï¼Œç›´åˆ°åˆ†æ•¸ç‚ºé›¶ã€‚
8. 'ç¸½åˆ†ä¸èƒ½è¶…éååˆ†'è¶…éä»¥ä»ååˆ†çµ±è¨ˆã€‚
9. 'ç¸½åˆ†ä¸èƒ½ä½æ–¼é›¶åˆ†ï¼Œä¹Ÿå°±æ˜¯ç¸½åˆ†ä¸èƒ½å‡ºç¾è² æ•¸'ä½æ–¼é›¶åˆ†ä»¥é›¶åˆ†çµ±è¨ˆã€‚
10. æ‰£åˆ†èˆ‡åŠ åˆ†å¿…å®šæ˜¯'æ•´æ•¸'ã€‚
11. 'è©•åˆ†æ™‚ï¼Œè‹¥[ä½¿ç”¨è€…çš„å›ç­”]æœ‰éŒ¯èª¤ï¼Œä¸¦åœ¨[æ‰£åˆ†åŸå› ]ä¾åºé»å‡ºéŒ¯èª¤çš„åœ°æ–¹ï¼Œçµ¦äºˆæ­£ç¢ºçš„ç­”æ¡ˆ'**å…§å®¹æœ‰éŒ¯èª¤ï¼Œå°±ä¸€å®šæœƒè¢«æ‰£åˆ†ï¼Œä¸å¯èƒ½æ»¿åˆ†ååˆ†**ã€‚

### å•é¡Œ
{question}

### ä½¿ç”¨è€…çš„å›ç­”
{user_answer}

è«‹æ ¹æ“š**è©•åˆ†æ¨™æº–**å°[ä½¿ç”¨è€…çš„å›ç­”]æ‰“ 1~10 åˆ†ï¼Œ1åˆ†æ˜¯æœ€ä½åˆ†ã€10åˆ†æ˜¯æœ€é«˜åˆ†ã€‚
ä¸¦ä¸”ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæä¾›ç°¡çŸ­(2ã€3å¥)çš„è©•èªï¼Œå‰©ä¸‹åªéœ€èªªæ˜æ‰£åˆ†åŸå› (æ²’æ‰£åˆ†ï¼Œä¹Ÿå°±æ˜¯å›ç­”æ»¿åˆ†ååˆ†å‰‡å¯«'æ‰£åˆ†åŸå› :ç„¡')ã€‚
è‹¥å­¸ç”Ÿçš„å›ç­”æœ‰éŒ¯æˆ–èªªæ˜ä¸è¶³çš„éƒ¨åˆ†ï¼Œè«‹åœ¨[æ‰£åˆ†åŸå› ]å¾Œå¯«ä¸Š'éŒ¯èª¤åœ°æ–¹èˆ‡æ­£ç¢ºçš„ç­”æ¡ˆ'æˆ–'é‡å°èªªæ˜ä¸è¶³çš„éƒ¨åˆ†æä¾›æ”¹å–„å»ºè­°'ï¼Œä¸¦åœ¨è©•èªçµ¦äºˆæ•´é«”å›ç­”è©•åƒ¹ã€‚

### è¼¸å‡ºæ ¼å¼ç‚º:
åˆ†æ•¸: (ç²å¾—åˆ†æ•¸)/10 åˆ†  
è©•èª: (è©•èª)  
æ‰£åˆ†åŸå› : (åˆ—é»æ‰£åˆ†åŸå› ï¼Œæ²’æœ‰æ‰£åˆ†å¯«'ç„¡')
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ è³‡æ–™çµæ§‹ è©•å¯©ï¼Œè² è²¬è©•åˆ†ä½¿ç”¨è€…çš„ç­”æ¡ˆã€‚"},
            {"role": "user", "content": prompt}
        ]
    )

    _loading = False
    t.join()
    print("\r" + " " * (len(message) + 5), end="")  # æ¸…ç©ºå‹•ç•«è¡Œ

    return response.choices[0].message.content.strip()

# ç”¨gpt-4o-miniåˆ¤æ–·å•é¡Œé¡å‹ï¼Œç”¨enumæ ¼å¼å›ç­”
# å¾Œè‡ºè·‘ä¸ç”¨å‘Šè¨´ä½¿ç”¨è€…ï¼Œç›´æ¥å›å‚³å•é¡Œé¡å‹
# å‰ç«¯èˆ‡å…¶ä»–åŠŸèƒ½ä¸ç”¨ç­‰å¾…(å¹³è¡Œè™•ç†)
def classify_question_type(question):
    global _loading
    _loading = True
    message = f"ğŸ“å•é¡Œé¡å‹åˆ†é¡ä¸­ï¼Œè«‹è€å¿ƒç­‰å¾…"
    t = threading.Thread(target=_animate_loading, args=(message,))
    t.start()

    prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è³‡æ–™çµæ§‹èª²ç¨‹åŠ©ç†ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å•é¡Œå…§å®¹ï¼Œåˆ¤æ–·å…¶é¡å‹ä¸¦å›å‚³å°æ‡‰çš„æšèˆ‰å€¼ã€‚
    å•é¡Œå…§å®¹ï¼š
    {question}
    è«‹æ ¹æ“šä»¥ä¸‹é¡å‹é€²è¡Œåˆ†é¡ï¼Œä¸¦ä»¥æšèˆ‰æ ¼å¼å›å‚³ï¼š
    - CHOICE: é¸æ“‡é¡Œ
    - DEFINITION: å®šç¾©é¡Œ
    - CALCULATION: è¨ˆç®—é¡Œ
    - SHORT_ANSWER: ç°¡ç­”é¡Œ
    - SITUATIONAL: æƒ…å¢ƒé¡Œ
    - OPEN_ENDED: é–‹æ”¾å¼å•é¡Œ
    - COMPREHENSIVE: ç¶œåˆæ€è€ƒé¡Œ
    å›å‚³æ ¼å¼ç‚ºï¼š
    qaType.XXX
    å…¶ä¸­ XXX ç‚ºå°æ‡‰çš„æšèˆ‰å€¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è³‡æ–™çµæ§‹èª²ç¨‹åŠ©ç†ï¼Œè² è²¬åˆ¤æ–·å•é¡Œé¡å‹ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )

    _loading = False
    t.join()
    print("\r" + " " * (len(message) + 5), end="")  # æ¸…ç©ºå‹•ç•«è¡Œ

    return response.choices[0].message.content.strip()


